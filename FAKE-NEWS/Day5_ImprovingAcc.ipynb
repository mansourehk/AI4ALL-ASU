{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='SEAGREEN'>Day 5</font>\n",
    "# <font color='MEDIUMSEAGREEN'>Improving the Accuracy</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two learning algorithms have been implemented so far.\n",
    "\n",
    "Now, let's look if we can improve the accuracy by having more nicer data!\n",
    "\n",
    "## Data\n",
    "Going back to the data, from our previous research we know that we can use stemming and lemmatizing to improve the collection of our words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "Remember that the goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "A difference between stemming and lemmatization is that stemming looks at the current word only, while lemmatization also takes the context into consideration. Either way, this pre-processing step could be somewhat tedious. Luckily, the powerful `nltk` provides tools for both.\n",
    "\n",
    "#### Stemming using the Porter stemmer\n",
    "*Porter's algorithm*, developed in the 1980s, is one of the most commonly used stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# Get the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Let's stemming on plurals\n",
    "plurals = ['apples', 'batteries', 'generators', 'medicines', 'tests', 'feet']\n",
    "print('plurals:')\n",
    "for plural in plurals:\n",
    "    print('{:s} --> {:s}'.format(plural, stemmer.stem(plural)))\n",
    "print()\n",
    "    \n",
    "# and variations of verbs\n",
    "verbs = ['studies', 'thinks', 'goes', 'played', 'bought', 'went', 'ran', 'drew', ]\n",
    "print('verbs:')\n",
    "for verb in verbs:\n",
    "    print('{:s} --> {:s}'.format(verb, stemmer.stem(verb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more words to `plurals` and see what the stemming results look like.  \n",
    "You may find that the results may look a bit mechanical. This is because the Porter's algorithm is essentially a sequential application of a set of rules. To get better looking results, let's try out a lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the following line when you this cell for the first time:\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Get the lemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the plurals\n",
    "print('plurals:')\n",
    "for plural in plurals:\n",
    "    print('{:s} --> {:s}'.format(plural, lmtzr.lemmatize(plural)))\n",
    "print()\n",
    "\n",
    "# Lemmatize the verbs\n",
    "print('verbs:')\n",
    "for verb in verbs:\n",
    "    print('{:s} --> {:s}'.format(verb, lmtzr.lemmatize(verb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not yet perfect, but much better, especially for the plurals. Whoray! :)\n",
    "\n",
    "Let's check whether stemming or lemmatization would improve our classification accuracy or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the tf values, use either lemmatization or stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export your result table to ``X2.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Naive Bayes Classifier on the new data and report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Exercise:** Synonyms\n",
    "\n",
    "It would be really cool to be able to check if two words are synonyms when comparing them. NLTK's WordNet (a corpus) allows us to find synsets (synonym sets) which we can use to do just that. Let's try to write a function to check whether two words are synonyms. \n",
    "\n",
    "Get help from [HERE](https://www.geeksforgeeks.org/get-synonymsantonyms-nltk-wordnet-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet \n",
    "\n",
    "def synonym_check(word1, word2):\n",
    "    # TODO: get all synsets from word 1\n",
    "    \n",
    "        # TODO: get lemmas for this synset\n",
    "        \n",
    "            # TODO: compare the name for this lemma to word 2\n",
    "            \n",
    "                \n",
    "                # TODO: return True if the same \n",
    "    \n",
    "    # TODO: otherwise, return false\n",
    "\n",
    "print(synonym_check(\"intelligence\", \"understanding\"))\n",
    "print(synonym_check(\"machine\", \"automation\"))\n",
    "print(synonym_check(\"robot\", \"human\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get False False False. \n",
    "\n",
    "Hmmm.... we can see that this method isn't as robust as we might like. Another way to check synonyms is to compare similarity indices, and then set a threshold for calling two words synonyms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def synonym_check_2(word1, word2):\n",
    "    # the maximum similarity found so far\n",
    "    max_wup = 0\n",
    "    # gets all possible synsets for word1\n",
    "    w1 = wordnet.synsets(word1)\n",
    "    # gets all possible synsets for word2\n",
    "    w2 = wordnet.synsets(word2) # n denotes noun\n",
    "    # TODO: for synset in w1\n",
    "    \n",
    "        # TODO: for synset in w2\n",
    "        \n",
    "            # TODO: get wup_similarity between the two\n",
    "            \n",
    "            \n",
    "            # TODO: if wup_sumilarity is greater than the previous maximum, update it\n",
    "\n",
    "    threshold = # TODO: set threshold\n",
    "    if max_wup >= threshold:\n",
    "        return max_wup, True\n",
    "    else:\n",
    "        return max_wup, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synonym_check_2(\"intelligence\", \"understanding\"))\n",
    "print(synonym_check_2(\"machine\", \"automation\"))\n",
    "print(synonym_check_2(\"robot\", \"human\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Exercise:**\n",
    "\n",
    "Try to improve the data with using the synonym_check_2 function. Report the accuracy with Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Exercise (Optional):**\n",
    "\n",
    "Some English words occur together more frequently. For example - Sky High, best performance, heavy rain. So, in a text document we may need to identify such pair of words which will help in sentiment analysis. First, we need to generate such word pairs from the existing sentence maintain their current sequences. Such pairs are called bigrams. Python has a bigram function as part of NLTK library which helps us generate these pairs.\n",
    "\n",
    "Find the bi-grams on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Now, you can call me a scientist! Drop the MIC!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
