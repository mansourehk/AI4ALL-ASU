{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='SEAGREEN'>Day 2</font>\n",
    "# <font color='MEDIUMSEAGREEN'>Exploring the Data</font>\n",
    "\n",
    "We can’t communicate with computers the same way we communicate to humans. \n",
    "\n",
    "- We need to do some extra work to describe a text to a computer.\n",
    "- We have to pick and choose what about that text we want to tell our computers.\n",
    "\n",
    "The measurable properties that we use to describe our examples/samples in our dataset are called ***features***.\n",
    "\n",
    "Today we will learn how we can extract these features.\n",
    "\n",
    "## Text Representation\n",
    "The most common way to model documents is to transform them into sparse numeric vectors. This representation is called “Bag of Words”. \n",
    "\n",
    "Today we will learn two methods:\n",
    "1. Vector space model\n",
    "2. TF-IDF\n",
    "\n",
    "### 1. Vector Space Model\n",
    "\n",
    "In the vector space model, we are given a set of documents D. Each document is a set of words. The goal is to convert these textual documents to feature vectors. We can represent document $i$ with vector $d_i$.\n",
    "\n",
    "$$d_i = (w_{1,i}, w_{2,i}, ... , w_{N,i}),$$ \n",
    "\n",
    "where N is the number of words used for vectorization and $w_{j,i}$ is the weight for word j that occurs in document i.\n",
    "\n",
    "Design choice for weights:\n",
    "    1. We can set it to 1 when the word j exists in document i and 0 when it does not.\n",
    "    2. We can also set this weight to the number of times the word j is observed in document i.\n",
    "**Example 1:** Suppose we have 3 documents d1, d2 and d3 in our dataset with the following contents:\n",
    "    - d1: It was the best of times\n",
    "    - d2: It was the worst of times\n",
    "    - d3: It was the age of wisdom\n",
    "    - d4: It was the age of foolishness\n",
    "(The above sentences are the first few lines of the book “A Tale of Two Cities” by Charles Dickens)\n",
    "\n",
    "How many distinct words does the dataset has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table that the columns are the distinct words of the documents and the rows are the number of the documents (Number of columns should be your calculated number from previous question and number of rows are 4, since we only have four documents).\n",
    "Fill out the cells of the table based on the above design choices.\n",
    "\n",
    "*Show your work to the instructor.*\n",
    "\n",
    "One problem with the above method is that commonly used words (such as “the”, “a”, “an”, “and”) will get more weight than other words. So one pre-processing on data is to delete these common words from the vector representation.\n",
    "\n",
    "How many stop words does the dataset in example 1 has? List them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create similar tables without the stopwords.\n",
    "\n",
    "*Show your work to the instructor.*\n",
    "\n",
    "### 2. TF-IDF\n",
    "A more generalized approach than the vector space model is to use the **term frequency-inverse document frequency (TF-IDF)** weighting scheme. In the TF-IDF scheme, $w_{j,i}$ is calculated as,\n",
    "\n",
    "$$w_{j,i} = tf_{j,i} × idf_j$$\n",
    "\n",
    "where $tf_{j,i}$ is the frequency of word j in document i. $idf_j$ is the inverse TF-IDF frequency of word j across all documents,\n",
    "\n",
    "$$ idf_j = \\log_2\\frac{|D|}{|\\{document \\in D : j \\in document|\\}}$$\n",
    "\n",
    "which is the logarithm of the total number of documents divided by the number of documents that contain word j. TF-IDF assigns higher weights to words that are less frequent across documents and, at the same time, have higher frequencies within the document they are used. This guarantees that words with high TF-IDF values can be used as representative examples of the documents they belong to and also, that stop words, which are common in all documents, are assigned smaller weights.\n",
    "\n",
    "**Example 2:** Consider the words “apple” and “orange” that appear 10 and 20 times in document d1. Let |D| = 20 and assume the word “apple” only appears in document d1 and the word “orange” appears in all 20 documents. Calculate the TF-IDF values for “apple” and “orange” in document d1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf(\"apple\", d1):\n",
    "# tf-idf(\"orange\", d1):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Consider the table you provided in *Example 1* (without stopwords). This table is the tf values. \n",
    "- Calculate the idf values for each word.\n",
    "- The TF-IDF values can be computed by multiplying tf values with the idf values (The result should be a table).\n",
    "\n",
    "*Show your work to the instructor.*\n",
    "\n",
    "After vectorization, documents are converted to vectors, and common data mining algorithms can be applied.\n",
    "\n",
    "## Dataset Pre-processing\n",
    "Now lets go back to our dataset and create the tf-idf values for our documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a list of stopwords. Don't worry, you don't need to think about different possible stopwords. Python has a library called \"nltk\" which stands for *Natural Language Toolkit* that has a list of stopwords stored in 16 different languages.\n",
    "\n",
    "To check the list of stopwords you can run the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even modify the list by adding words of your choice to the ``stop_words`` set.\n",
    "\n",
    "Now consider we want to remove the stop words from the following text:\n",
    "\n",
    "    Through the Mayo Clinic and ASU Alliance for Health Care Faculty Summer Residency Program, six professors from the College of Health Solutions and Ira A. Fulton Schools of Engineering will spend six weeks working side by side with Mayo Clinic researchers at a Mayo Clinic site in either Rochester, Minnesota, or locally in Phoenix or Scottsdale. The teams will collaborate on research that seeks to have a direct impact on patient outcomes and experiences. This year’s cohort is tackling questions relating to Alzheimer’s disease, Type 1 diabetes, liver disease and more.\n",
    "\n",
    "First, we need to convert the text to list of words. nltk helps us to do so by tokenizing our sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence = \"Through the Mayo Clinic and ASU Alliance for Health Care Faculty Summer Residency Program, six professors from the College of Health Solutions and Ira A. Fulton Schools of Engineering will spend six weeks working side by side with Mayo Clinic researchers at a Mayo Clinic site in either Rochester, Minnesota, or locally in Phoenix or Scottsdale. The teams will collaborate on research that seeks to have a direct impact on patient outcomes and experiences. This year’s cohort is tackling questions relating to Alzheimer’s disease, Type 1 diabetes, liver disease and more.\"\n",
    "sentence = sentence.lower()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, It is time to remove the stopwords from the tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_wo_stopwords = [] \n",
    "  \n",
    "for w in words: \n",
    "    if w not in stop_words: \n",
    "        words_wo_stopwords.append(w)\n",
    "print(words_wo_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the same for our data.\n",
    "\n",
    "RECALL: you saved your data in ``data.csv`` file.\n",
    "\n",
    "load your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all the texts from all the articles together (because we want to find all the words used in our dataset). Use print to print out your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data and remove the stopwords. Don't forget to lower case all the words. Delete the duplicates from the list (Search and see how you can do this). Print out the number of the words before deleting the stopwords, before deleting the duplicates and at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to create the tf table, what would be the number of columns and the number of the rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tf table.\n",
    "    - Create an empty dataframe filled with values zero.\n",
    "    - Count the words in each article and fill out the created dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the idf values for each word. To use logarithm in python you should import math library. \n",
    "\n",
    "``math.log(a,Base)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the TF-TDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export your results to ``X.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research\n",
    "Is there any way in python that you can improve the collection of the words?\n",
    "\n",
    "For example, the two words \"<font color='INDIGO'>dogs</font>\" and \"<font color='INDIGO'>dog</font>\" or \"<font color='ORCHID'>walk</font>\" and \"<font color='ORCHID'>walking</font>\" are not that different, but the above method counts them as two seperate words.\n",
    "\n",
    "HINT: The process of producing morphological variants of a root/base word is called *Stemming*. *Lemmatization* is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "\n",
    "Write down the result of your research in the below block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Exercise:** We can calculate the tf-idf values with scikit-learn. Find out how we can do that and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nice work today!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    - Zafarani, Reza, Mohammad Ali Abbasi, and Huan Liu. Social media mining: an introduction. Cambridge University Press, 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
